{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "import requests, datetime, time, h5py, json\n",
    "from sklearn.externals import joblib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = joblib.load('y_trainLasVegas.joblib')\n",
    "X_train = joblib.load('X_trainLasVegas.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating y_train for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "y_train.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      1784.000000\n",
       "mean      25769.674888\n",
       "std       14656.852380\n",
       "min        2674.000000\n",
       "25%       18137.000000\n",
       "50%       22337.000000\n",
       "75%       27833.000000\n",
       "max      142100.000000\n",
       "Name: income, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landarea</th>\n",
       "      <th>waterarea</th>\n",
       "      <th>0_restaurants_near</th>\n",
       "      <th>0_restaurants_far</th>\n",
       "      <th>1_restaurants_near</th>\n",
       "      <th>1_restaurants_far</th>\n",
       "      <th>2_restaurants_near</th>\n",
       "      <th>2_restaurants_far</th>\n",
       "      <th>3_restaurants_near</th>\n",
       "      <th>3_restaurants_far</th>\n",
       "      <th>...</th>\n",
       "      <th>0_physicaltherapy_near</th>\n",
       "      <th>0_physicaltherapy_far</th>\n",
       "      <th>1_physicaltherapy_near</th>\n",
       "      <th>1_physicaltherapy_far</th>\n",
       "      <th>2_physicaltherapy_near</th>\n",
       "      <th>2_physicaltherapy_far</th>\n",
       "      <th>3_physicaltherapy_near</th>\n",
       "      <th>3_physicaltherapy_far</th>\n",
       "      <th>4_physicaltherapy_near</th>\n",
       "      <th>4_physicaltherapy_far</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1482 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   landarea  waterarea  0_restaurants_near  0_restaurants_far  \\\n",
       "0         0          0                   0                  1   \n",
       "1         0          0                   0                  1   \n",
       "2         0          0                   0                  3   \n",
       "3         0          0                   0                  2   \n",
       "4         0          0                   0                 35   \n",
       "\n",
       "   1_restaurants_near  1_restaurants_far  2_restaurants_near  \\\n",
       "0                   8                 37                   4   \n",
       "1                   0                 31                   0   \n",
       "2                   3                 22                   3   \n",
       "3                   0                 22                   1   \n",
       "4                   0                 71                   0   \n",
       "\n",
       "   2_restaurants_far  3_restaurants_near  3_restaurants_far  \\\n",
       "0                 26                   0                  0   \n",
       "1                 22                   0                  0   \n",
       "2                 16                   0                  0   \n",
       "3                 12                   0                  8   \n",
       "4                 11                   0                  3   \n",
       "\n",
       "           ...            0_physicaltherapy_near  0_physicaltherapy_far  \\\n",
       "0          ...                                 0                      0   \n",
       "1          ...                                 0                      0   \n",
       "2          ...                                 1                      3   \n",
       "3          ...                                 0                      3   \n",
       "4          ...                                 0                      3   \n",
       "\n",
       "   1_physicaltherapy_near  1_physicaltherapy_far  2_physicaltherapy_near  \\\n",
       "0                       0                      0                       0   \n",
       "1                       0                      0                       0   \n",
       "2                       0                      0                       0   \n",
       "3                       0                      0                       0   \n",
       "4                       0                      0                       0   \n",
       "\n",
       "   2_physicaltherapy_far  3_physicaltherapy_near  3_physicaltherapy_far  \\\n",
       "0                      0                       0                      0   \n",
       "1                      0                       0                      0   \n",
       "2                      0                       0                      0   \n",
       "3                      0                       0                      0   \n",
       "4                      0                       0                      0   \n",
       "\n",
       "   4_physicaltherapy_near  4_physicaltherapy_far  \n",
       "0                       0                      0  \n",
       "1                       0                      0  \n",
       "2                       0                      0  \n",
       "3                       0                      0  \n",
       "4                       0                      0  \n",
       "\n",
       "[5 rows x 1482 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating column y_train_class\n",
    "\n",
    "y_train_class = pd.Series(['NaN' for i in range(len(y_train)) ])\n",
    "\n",
    "## Creating y_train for classification\n",
    "\n",
    "brackets ={'$0-$10.99k': 1,'$11-$20.99k':2, '$21-$30.99k':3, '$31-$40.99k':4, '$41-$50.99k':5, '$51-$60.99k':6,\n",
    "            '$61-$70.99k':7, '$71-$80.99k':8, '$81+k.99':9}\n",
    "\n",
    "for index, value in enumerate(y_train):\n",
    "    if y_train.iloc[index] < 11000:\n",
    "        y_train_class.iloc[index] = brackets['$0-$10.99k']\n",
    "    elif y_train.iloc[index] < 21000:\n",
    "        y_train_class.iloc[index] = brackets['$11-$20.99k']\n",
    "    elif y_train.iloc[index] < 31000:\n",
    "        y_train_class.iloc[index] = brackets['$21-$30.99k']\n",
    "    elif y_train.iloc[index] < 41000:\n",
    "        y_train_class.iloc[index] = brackets['$31-$40.99k']   \n",
    "    elif y_train.iloc[index] < 51000:\n",
    "        y_train_class.iloc[index] = brackets['$41-$50.99k']\n",
    "    elif y_train.iloc[index] < 61000:\n",
    "        y_train_class.iloc[index] = brackets['$51-$60.99k']\n",
    "    elif y_train.iloc[index] < 71000:\n",
    "        y_train_class.iloc[index] = brackets['$61-$70.99k']   \n",
    "    elif y_train.iloc[index] < 81000:\n",
    "        y_train_class.iloc[index] = brackets['$71-$80.99k']\n",
    "    else:\n",
    "        y_train_class.iloc[index] = brackets['$81+k.99']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking mapping output\n",
    "#df_y_train = pd.DataFrame(y_train)\n",
    "#df_y_train_class = pd.DataFrame(y_train_class)\n",
    "#pd.concat([df_y_train, df_y_train_class]\n",
    " #         , axis=1, index=).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_class.to_csv('y_train_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiating RandomForest model.\n",
    "rfc = RandomForestClassifier(n_estimators = 100,\n",
    "    max_depth= 15,\n",
    "    max_features= 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.59 s, sys: 7.05 ms, total: 2.59 s\n",
      "Wall time: 2.59 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9065895303691696"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "cross_val_score(rfc, X_train, y_train_class, cv = 5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Performing Grid Searching\n",
    "rfc = RandomForestClassifier()   #you could add random_state, but you don0't want to tune around a rspecific random state.\n",
    "rf_params = {\n",
    "    'n_estimators': [43, 45, 47],\n",
    "    'max_depth': [None, 2, 3, 4],\n",
    "    'max_features': ['auto']                #when on auto is the square root of number of rows. \n",
    "                                                 #['auto', 1.0] if you show each tree 100% of ... you are doing a bagging classifier!\n",
    "}\n",
    "gs = GridSearchCV(rfc, param_grid=rf_params, cv=5)\n",
    "gs.fit(X_train, y_train_class)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_estimators = 100,\n",
    "    max_depth= 10,\n",
    "    max_features= 'auto')\n",
    "#rf_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.8 s, sys: 116 ms, total: 54.9 s\n",
      "Wall time: 55 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7119831403646734"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cross_val_score(rfr, X_train, y_train, cv = 5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7825449858967413\n",
      "CPU times: user 14min 59s, sys: 1.01 s, total: 15min\n",
      "Wall time: 15min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Performing Grid Searching\n",
    "rfr = RandomForestRegressor()   #you could add random_state, but you don0't want to tune around a rspecific random state.\n",
    "rfr_params = {\n",
    "    'n_estimators': [40, 60, 80, 100],\n",
    "    'max_depth': [None, 2, 3, 4, 10, 12, 15],\n",
    "    'max_features': ['auto']                #when on auto is the square root of number of rows. \n",
    "                                                 #['auto', 1.0] if you show each tree 100% of ... you are doing a bagging classifier!\n",
    "}\n",
    "gs = GridSearchCV(rfr, param_grid=rfr_params, cv=5)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'max_features': 'auto', 'n_estimators': 100}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.15, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype uint16 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype uint16 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype uint16 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_models = [linear_reg, knn_reg, cart_reg, bagged_reg, adaboost_reg, random_forest_reg, support_vector_reg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "knn_reg = KNeighborsRegressor()\n",
    "knn_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "cart_reg = DecisionTreeRegressor()\n",
    "cart_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "bagged_reg = BaggingRegressor()\n",
    "bagged_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "adaboost_reg = AdaBoostRegressor()\n",
    "adaboost_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "random_forest_reg = RandomForestRegressor()\n",
    "random_forest_reg.fit(X_train, y_train)\n",
    "\n",
    "support_vector_reg = SVR()\n",
    "support_vector_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse_score(model, X_train, X_test, y_train, y_test):\n",
    "    mse_train = mean_squared_error(y_true = y_train,\n",
    "                                  y_pred = model.predict(X_train))\n",
    "    mse_test = mean_squared_error(y_true = y_test,\n",
    "                                  y_pred = model.predict(X_test))\n",
    "    rmse_train = mse_train ** 0.5\n",
    "    rmse_test = mse_test ** 0.5\n",
    "    \n",
    "    print(\"The training RMSE for \" + str(model) + \" is: \" + str(rmse_train))\n",
    "    print(\"The testing RMSE for \" + str(model) + \" is: \" + str(rmse_test))\n",
    "    return (rmse_train, rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False) is: 2.6427887491002686e-13\n",
      "The testing RMSE for LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False) is: 13.214595826817288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.6427887491002686e-13, 13.214595826817288)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(linear_reg, X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "          weights='uniform') is: 0.6076888301472344\n",
      "The testing RMSE for KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "          weights='uniform') is: 0.5695489135379689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6076888301472344, 0.5695489135379689)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(knn_reg, X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best') is: 0.0\n",
      "The testing RMSE for DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best') is: 0.7521897856819185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.7521897856819185)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(cart_reg, X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False) is: 0.27308054007281374\n",
      "The testing RMSE for BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False) is: 0.6267781722371819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.27308054007281374, 0.6267781722371819)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(bagged_reg, X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "         n_estimators=50, random_state=None) is: 0.848002560893862\n",
      "The testing RMSE for AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "         n_estimators=50, random_state=None) is: 0.8456140546402734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.848002560893862, 0.8456140546402734)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(adaboost_reg, X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
      "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
      "  tol=0.001, verbose=False) is: 0.813239105707988\n",
      "The testing RMSE for SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
      "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
      "  tol=0.001, verbose=False) is: 0.6248344103933527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.813239105707988, 0.6248344103933527)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(support_vector_reg, X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False) is: 2.500543419199984\n",
      "The testing RMSE for RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False) is: 2.4755027843021544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.500543419199984, 2.4755027843021544)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(random_forest_reg, X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**\n",
    "With default parameters, it seems that we should investigate further the following regression models:\n",
    "- **knn** (0.6076888301472344, 0.5695489135379689) #RMSE, (train, test)\n",
    "- **SVR** (0.813239105707988, 0.6248344103933527)\n",
    "- **Adaboost** (0.848002560893862, 0.8456140546402734) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.15, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype uint16 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype uint16 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype uint16 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying PCA to Xs\n",
    "pca = PCA()\n",
    "pca = pca.fit(X_train_scaled)\n",
    "Z_train = pca.transform(X_train_scaled)\n",
    "Z_test =pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance:             [0.1014 0.0534 0.0444 ... 0.     0.     0.    ]\n",
      "Cumulative explained variance:  [0.10144166 0.15482658 0.19924002 ... 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Examine explained variance.\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "print('Explained variance:            ', np.round(var_exp, 4))\n",
    "\n",
    "# Examine cumulative explained variance.\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print('Cumulative explained variance: ', cum_var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.989504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.989647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.989785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.989924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0.990059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.990194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0.990326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "260  0.989504\n",
       "261  0.989647\n",
       "262  0.989785\n",
       "263  0.989924\n",
       "264  0.990059\n",
       "265  0.990194\n",
       "266  0.990326"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Looking for the ith principal that combined with more important principals explains 99% of variance.\n",
    "cum_var_exp_df = pd.DataFrame(cum_var_exp)\n",
    "cum_var_exp_df[260:267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying PCA to Xs\n",
    "pca = PCA(n_components=264)\n",
    "pca = pca.fit(X_train_scaled)\n",
    "Z_train = pca.transform(X_train_scaled)\n",
    "Z_test =pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 264)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg_PCA = LinearRegression()\n",
    "linear_reg_PCA.fit(Z_train, y_train)\n",
    "\n",
    "knn_reg_PCA = KNeighborsRegressor()\n",
    "knn_reg_PCA.fit(Z_train, y_train)\n",
    "\n",
    "cart_reg_PCA = DecisionTreeRegressor()\n",
    "cart_reg_PCA.fit(Z_train, y_train)\n",
    "\n",
    "bagged_reg_PCA = BaggingRegressor()\n",
    "bagged_reg_PCA.fit(Z_train, y_train)\n",
    "\n",
    "adaboost_reg_PCA = AdaBoostRegressor()\n",
    "adaboost_reg_PCA.fit(Z_train, y_train)\n",
    "\n",
    "random_forest_reg_PCA = RandomForestRegressor()\n",
    "random_forest_reg_PCA.fit(Z_train, y_train)\n",
    "\n",
    "support_vector_reg_PCA = SVR()\n",
    "support_vector_reg_PCA.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False) is: 0.5330416024064061\n",
      "The testing RMSE for LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False) is: 0.838008473791076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5330416024064061, 0.838008473791076)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(linear_reg_PCA, Z_train, Z_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "          weights='uniform') is: 0.6647290550564173\n",
      "The testing RMSE for KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "          weights='uniform') is: 0.7212532048920202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6647290550564173, 0.7212532048920202)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(knn_reg_PCA, Z_train, Z_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best') is: 0.0\n",
      "The testing RMSE for DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best') is: 0.9109867766419526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.9109867766419526)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(cart_reg_PCA, Z_train, Z_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False) is: 0.3739946914736318\n",
      "The testing RMSE for BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False) is: 0.6250360814327309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3739946914736318, 0.6250360814327309)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(bagged_reg_PCA, Z_train, Z_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "         n_estimators=50, random_state=None) is: 0.8625511532481682\n",
      "The testing RMSE for AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "         n_estimators=50, random_state=None) is: 0.9652819915349301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8625511532481682, 0.9652819915349301)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(adaboost_reg_PCA, Z_train, Z_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False) is: 0.37399469147363185\n",
      "The testing RMSE for RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False) is: 0.6230950350311038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.37399469147363185, 0.6230950350311038)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(random_forest_reg_PCA, Z_train, Z_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training RMSE for SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
      "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
      "  tol=0.001, verbose=False) is: 0.7746690072748086\n",
      "The testing RMSE for SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
      "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
      "  tol=0.001, verbose=False) is: 0.8100704532710763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7746690072748086, 0.8100704532710763)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(support_vector_reg_PCA, Z_train, Z_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RMSE reg models with default parameters with PCA**\n",
    "- linear_reg_PCA - (0.5330416024064061, 0.838008473791076)\n",
    "- knn_reg_PCA - (0.6647290550564173, 0.7212532048920202)\n",
    "- cart_reg_PCA - (0.0, 0.9109867766419526)\n",
    "- bagged_reg_PCA - (0.3739946914736318, 0.6250360814327309)\n",
    "- adaboost_reg_PCA - (0.8625511532481682, 0.9652819915349301)\n",
    "- random_forest_reg_PCA - (0.37399469147363185, 0.6230950350311038)\n",
    "- support_vector_reg_PCA - (0.7746690072748086, 0.8100704532710763)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_reg_models = [linear_reg_PCA, knn_reg_PCA, cart_reg_PCA, bagged_reg_PCA, adaboost_reg_PCA, \n",
    "                  random_forest_reg_PCA, support_vector_reg_PCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model in pca_reg_models:\n",
    " #   joblib.dump(model, ('Models/Raw_Models/PCA/reg/'+model.__class__.__name__+'.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train_class, test_size=.15, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype uint16 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype uint16 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype uint16 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_models = [logreg_class, knn_class, cart_class, bagged_class, \n",
    "              random_forest_class, adaboost_class, support_vector_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_class = LogisticRegression()\n",
    "logreg_class.fit(X_train_scaled, y_train)\n",
    "\n",
    "knn_class = KNeighborsClassifier()\n",
    "knn_class.fit(X_train_scaled, y_train)\n",
    "\n",
    "cart_class = DecisionTreeClassifier()\n",
    "cart_class.fit(X_train_scaled, y_train)\n",
    "\n",
    "bagged_class = BaggingClassifier()\n",
    "bagged_class.fit(X_train_scaled, y_train)\n",
    "\n",
    "random_forest_class = RandomForestClassifier()\n",
    "random_forest_class.fit(X_train_scaled, y_train)\n",
    "\n",
    "adaboost_class = AdaBoostClassifier()\n",
    "adaboost_class.fit(X_train_scaled, y_train)\n",
    "\n",
    "support_vector_class = SVC()\n",
    "support_vector_class.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_scorer(model, X_train, X_test, y_train, y_test):\n",
    "    accuracy_train = f1_score(y_true = y_train,\n",
    "                        y_pred = model.predict(X_train), average=None)\n",
    "    accuracy_test = f1_score(y_true = y_test,\n",
    "                       y_pred = model.predict(X_test), average=None)\n",
    "    \n",
    "    print(\"The training accuracy-score for \" + str(model) + \" is: \" + str(accuracy_train))\n",
    "    print(\"The testing accuracy-score for \" + str(model) + \" is: \" + str(accuracy_test))\n",
    "    return (accuracy_train, accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy-score for LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) is: [1.         0.9894129  0.98432602 0.98867925 1.         0.98734177\n",
      " 1.         1.         1.        ]\n",
      "The testing accuracy-score for LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) is: [0.90909091 0.90721649 0.87573964 0.83636364 1.         0.8\n",
      " 0.         1.         0.5       ]\n",
      "([1.0, 0.9894128970163618, 0.9843260188087775, 0.9886792452830188, 1.0, 0.9873417721518987, 1.0, 1.0, 1.0], [0.9090909090909091, 0.9072164948453608, 0.8757396449704142, 0.8363636363636363, 1.0, 0.8, 0.0, 1.0, 0.5])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy-score for KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') is: [0.89855072 0.89760766 0.8556701  0.79681275 0.83544304 0.8045977\n",
      " 0.375      0.8        0.69387755]\n",
      "The testing accuracy-score for KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') is: [0.71428571 0.81443299 0.79069767 0.81632653 0.72727273 0.83333333\n",
      " 0.         0.         1.        ]\n",
      "([0.8985507246376813, 0.8976076555023923, 0.8556701030927835, 0.796812749003984, 0.8354430379746836, 0.8045977011494252, 0.37499999999999994, 0.8, 0.6938775510204082], [0.7142857142857143, 0.8144329896907218, 0.7906976744186046, 0.8163265306122449, 0.7272727272727272, 0.8333333333333334, 0.0, 0.0, 1.0])\n",
      "\n",
      "\n",
      "The training accuracy-score for DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') is: [1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "The testing accuracy-score for DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') is: [0.90909091 0.88659794 0.8313253  0.71428571 0.8        0.5\n",
      " 0.         1.         0.66666667]\n",
      "([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.9090909090909091, 0.88659793814433, 0.8313253012048192, 0.7142857142857142, 0.8000000000000002, 0.5, 0.0, 1.0, 0.6666666666666666])\n",
      "\n",
      "\n",
      "The training accuracy-score for BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False) is: [1.         0.99614644 0.99687174 0.99619772 0.98701299 0.98765432\n",
      " 1.         1.         1.        ]\n",
      "The testing accuracy-score for BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False) is: [1.         0.92079208 0.88757396 0.75       0.88888889 0.90909091\n",
      " 0.         1.         0.5       ]\n",
      "([1.0, 0.9961464354527938, 0.9968717413972888, 0.9961977186311787, 0.9870129870129869, 0.9876543209876543, 1.0, 1.0, 1.0], [1.0, 0.9207920792079209, 0.8875739644970414, 0.7500000000000001, 0.888888888888889, 0.9090909090909091, 0.0, 1.0, 0.5])\n",
      "\n",
      "\n",
      "The training accuracy-score for RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) is: [1.         0.99710145 0.99274611 0.97709924 1.         1.\n",
      " 0.95238095 0.90909091 1.        ]\n",
      "The testing accuracy-score for RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) is: [0.90909091 0.92610837 0.87272727 0.84       1.         0.76923077\n",
      " 0.         0.         1.        ]\n",
      "([1.0, 0.9971014492753624, 0.9927461139896372, 0.9770992366412214, 1.0, 1.0, 0.9523809523809523, 0.9090909090909091, 1.0], [0.9090909090909091, 0.9261083743842364, 0.8727272727272727, 0.8400000000000001, 1.0, 0.7692307692307692, 0.0, 0.0, 1.0])\n",
      "\n",
      "\n",
      "The training accuracy-score for AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None) is: [0.         0.48232323 0.57526882 0.         0.         0.\n",
      " 0.         1.         0.        ]\n",
      "The testing accuracy-score for AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None) is: [0.         0.44       0.54615385 0.         0.         0.\n",
      " 0.         1.         0.        ]\n",
      "([0.0, 0.4823232323232323, 0.5752688172043011, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.44, 0.5461538461538461, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy-score for SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False) is: [0.625      0.89373814 0.85798237 0.725      0.77142857 0.75362319\n",
      " 0.         1.         0.82352941]\n",
      "The testing accuracy-score for SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False) is: [0.28571429 0.83838384 0.81318681 0.69565217 0.66666667 0.5\n",
      " 0.         1.         0.66666667]\n",
      "([0.625, 0.8937381404174574, 0.8579823702252694, 0.7250000000000001, 0.7714285714285716, 0.7536231884057972, 0.0, 1.0, 0.823529411764706], [0.28571428571428575, 0.8383838383838383, 0.8131868131868131, 0.6956521739130435, 0.6666666666666665, 0.5, 0.0, 1.0, 0.6666666666666666])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_scorer(logreg_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(accuracy_scorer(knn_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(accuracy_scorer(cart_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(accuracy_scorer(bagged_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(accuracy_scorer(random_forest_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(accuracy_scorer(adaboost_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(accuracy_scorer(support_vector_class, X_train_scaled, X_test_scaled, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_scorer(model, X_train, X_test, y_train, y_test):\n",
    "    f1_train = f1_score(y_true = y_train,\n",
    "                        y_pred = model.predict(X_train), average=None)\n",
    "    f1_test = f1_score(y_true = y_test,\n",
    "                       y_pred = model.predict(X_test), average=None)\n",
    "    \n",
    "    print(\"The training F1-score for \" + str(model) + \" is: \" + str(f1_train))\n",
    "    print(\"The testing F1-score for \" + str(model) + \" is: \" + str(f1_test))\n",
    "    return (f1_train, f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training F1-score for LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) is: [1.         0.9894129  0.98432602 0.98867925 1.         0.98734177\n",
      " 1.         1.         1.        ]\n",
      "The testing F1-score for LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) is: [0.90909091 0.90721649 0.87573964 0.83636364 1.         0.8\n",
      " 0.         1.         0.5       ]\n",
      "(array([1.        , 0.9894129 , 0.98432602, 0.98867925, 1.        ,\n",
      "       0.98734177, 1.        , 1.        , 1.        ]), array([0.90909091, 0.90721649, 0.87573964, 0.83636364, 1.        ,\n",
      "       0.8       , 0.        , 1.        , 0.5       ]))\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training F1-score for KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') is: [0.89855072 0.89760766 0.8556701  0.79681275 0.83544304 0.8045977\n",
      " 0.375      0.8        0.69387755]\n",
      "The testing F1-score for KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') is: [0.71428571 0.81443299 0.79069767 0.81632653 0.72727273 0.83333333\n",
      " 0.         0.         1.        ]\n",
      "(array([0.89855072, 0.89760766, 0.8556701 , 0.79681275, 0.83544304,\n",
      "       0.8045977 , 0.375     , 0.8       , 0.69387755]), array([0.71428571, 0.81443299, 0.79069767, 0.81632653, 0.72727273,\n",
      "       0.83333333, 0.        , 0.        , 1.        ]))\n",
      "\n",
      "\n",
      "The training F1-score for DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') is: [1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "The testing F1-score for DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') is: [0.90909091 0.88659794 0.8313253  0.71428571 0.8        0.5\n",
      " 0.         1.         0.66666667]\n",
      "(array([1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([0.90909091, 0.88659794, 0.8313253 , 0.71428571, 0.8       ,\n",
      "       0.5       , 0.        , 1.        , 0.66666667]))\n",
      "\n",
      "\n",
      "The training F1-score for BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False) is: [1.         0.99614644 0.99687174 0.99619772 0.98701299 0.98765432\n",
      " 1.         1.         1.        ]\n",
      "The testing F1-score for BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False) is: [1.         0.92079208 0.88757396 0.75       0.88888889 0.90909091\n",
      " 0.         1.         0.5       ]\n",
      "(array([1.        , 0.99614644, 0.99687174, 0.99619772, 0.98701299,\n",
      "       0.98765432, 1.        , 1.        , 1.        ]), array([1.        , 0.92079208, 0.88757396, 0.75      , 0.88888889,\n",
      "       0.90909091, 0.        , 1.        , 0.5       ]))\n",
      "\n",
      "\n",
      "The training F1-score for RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) is: [1.         0.99710145 0.99274611 0.97709924 1.         1.\n",
      " 0.95238095 0.90909091 1.        ]\n",
      "The testing F1-score for RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) is: [0.90909091 0.92610837 0.87272727 0.84       1.         0.76923077\n",
      " 0.         0.         1.        ]\n",
      "(array([1.        , 0.99710145, 0.99274611, 0.97709924, 1.        ,\n",
      "       1.        , 0.95238095, 0.90909091, 1.        ]), array([0.90909091, 0.92610837, 0.87272727, 0.84      , 1.        ,\n",
      "       0.76923077, 0.        , 0.        , 1.        ]))\n",
      "\n",
      "\n",
      "The training F1-score for AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None) is: [0.         0.48232323 0.57526882 0.         0.         0.\n",
      " 0.         1.         0.        ]\n",
      "The testing F1-score for AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None) is: [0.         0.44       0.54615385 0.         0.         0.\n",
      " 0.         1.         0.        ]\n",
      "(array([0.        , 0.48232323, 0.57526882, 0.        , 0.        ,\n",
      "       0.        , 0.        , 1.        , 0.        ]), array([0.        , 0.44      , 0.54615385, 0.        , 0.        ,\n",
      "       0.        , 0.        , 1.        , 0.        ]))\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training F1-score for SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False) is: [0.625      0.89373814 0.85798237 0.725      0.77142857 0.75362319\n",
      " 0.         1.         0.82352941]\n",
      "The testing F1-score for SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False) is: [0.28571429 0.83838384 0.81318681 0.69565217 0.66666667 0.5\n",
      " 0.         1.         0.66666667]\n",
      "(array([0.625     , 0.89373814, 0.85798237, 0.725     , 0.77142857,\n",
      "       0.75362319, 0.        , 1.        , 0.82352941]), array([0.28571429, 0.83838384, 0.81318681, 0.69565217, 0.66666667,\n",
      "       0.5       , 0.        , 1.        , 0.66666667]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(f1_scorer(logreg_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer(knn_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer(cart_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer(bagged_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer(random_forest_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer(adaboost_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer(support_vector_class, X_train_scaled, X_test_scaled, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_scorer_micro(model, X_train, X_test, y_train, y_test):\n",
    "    f1_train = f1_score(y_true = y_train,\n",
    "                        y_pred = model.predict(X_train), average='micro')\n",
    "    f1_test = f1_score(y_true = y_test,\n",
    "                       y_pred = model.predict(X_test), average='micro')\n",
    "    \n",
    "    print(\"The training F1-score-micro for \" + str(model) + \" is: \" + str(f1_train))\n",
    "    print(\"The testing F1-score-micro for \" + str(model) + \" is: \" + str(f1_test))\n",
    "    return (f1_train, f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training F1-score-micro for LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) is: 0.9883540372670807\n",
      "The testing F1-score-micro for LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) is: 0.881578947368421\n",
      "(0.9883540372670807, 0.881578947368421)\n",
      "\n",
      "\n",
      "The training F1-score-micro for KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') is: 0.8594720496894411\n",
      "The testing F1-score-micro for KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') is: 0.7982456140350878\n",
      "(0.8594720496894411, 0.7982456140350878)\n",
      "\n",
      "\n",
      "The training F1-score-micro for DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') is: 1.0\n",
      "The testing F1-score-micro for DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') is: 0.8289473684210527\n",
      "(1.0, 0.8289473684210527)\n",
      "\n",
      "\n",
      "The training F1-score-micro for BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False) is: 0.9961180124223602\n",
      "The testing F1-score-micro for BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False) is: 0.8859649122807017\n",
      "(0.9961180124223602, 0.8859649122807017)\n",
      "\n",
      "\n",
      "The training F1-score-micro for RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) is: 0.9930124223602484\n",
      "The testing F1-score-micro for RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) is: 0.8903508771929824\n",
      "(0.9930124223602484, 0.8903508771929824)\n",
      "\n",
      "\n",
      "The training F1-score-micro for AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None) is: 0.48524844720496896\n",
      "The testing F1-score-micro for AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None) is: 0.4605263157894737\n",
      "(0.48524844720496896, 0.4605263157894737)\n",
      "\n",
      "\n",
      "The training F1-score-micro for SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False) is: 0.8470496894409938\n",
      "The testing F1-score-micro for SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False) is: 0.793859649122807\n",
      "(0.8470496894409938, 0.793859649122807)\n"
     ]
    }
   ],
   "source": [
    "print(f1_scorer_micro(logreg_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer_micro(knn_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer_micro(cart_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer_micro(bagged_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer_micro(random_forest_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer_micro(adaboost_class, X_train_scaled, X_test_scaled, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer_micro(support_vector_class, X_train_scaled, X_test_scaled, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**\n",
    "With default parameters, it seems that we should investigate further the following classification models:\n",
    "- **knn** (0.8594720496894411, 0.7982456140350878) #RMSE, (train, test)\n",
    "- **SVC** (0.8470496894409938, 0.793859649122807)\n",
    "- **RandomForest** (0.9930124223602484, 0.8903508771929824)\n",
    "- **LogisticRegression** (0.9883540372670807, 0.881578947368421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adaboost_class_lv.joblib']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib.dump(adaboost_class, 'adaboost_class_lv.joblib', compress=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving models with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model in class_models:\n",
    " #   joblib.dump(model, ('Models/Raw_Models/class/'+model.__class__.__name__+'.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model in reg_models:\n",
    " #   joblib.dump(model, ('Models/Raw_Models/reg/'+model.__class__.__name__+'.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_reg_models = [logreg_class_pca, knn_class_pca, cart_class_pca, bagged_class_pca, \n",
    "              random_forest_class_pca, adaboost_class_pca, support_vector_class_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_class_pca = LogisticRegression()\n",
    "logreg_class_pca.fit(Z_train, y_train)\n",
    "\n",
    "knn_class_pca = KNeighborsClassifier()\n",
    "knn_class_pca.fit(Z_train, y_train)\n",
    "\n",
    "cart_class_pca = DecisionTreeClassifier()\n",
    "cart_class_pca.fit(Z_train, y_train)\n",
    "\n",
    "bagged_class_pca = BaggingClassifier()\n",
    "bagged_class_pca.fit(Z_train, y_train)\n",
    "\n",
    "random_forest_class_pca = RandomForestClassifier()\n",
    "random_forest_class_pca.fit(Z_train, y_train)\n",
    "\n",
    "adaboost_class_pca = AdaBoostClassifier()\n",
    "adaboost_class_pca.fit(Z_train, y_train)\n",
    "\n",
    "support_vector_class_pca = SVC()\n",
    "support_vector_class_pca.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training F1-score-micro for LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) is: 0.9725776965265083\n",
      "The testing F1-score-micro for LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) is: 0.8556701030927835\n",
      "(0.9725776965265083, 0.8556701030927835)\n",
      "\n",
      "\n",
      "The training F1-score-micro for KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') is: 0.8510054844606947\n",
      "The testing F1-score-micro for KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') is: 0.7938144329896907\n",
      "(0.8510054844606947, 0.7938144329896907)\n",
      "\n",
      "\n",
      "The training F1-score-micro for DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') is: 1.0\n",
      "The testing F1-score-micro for DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') is: 0.7731958762886598\n",
      "(1.0, 0.7731958762886598)\n",
      "\n",
      "\n",
      "The training F1-score-micro for BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False) is: 0.9926873857404022\n",
      "The testing F1-score-micro for BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False) is: 0.8350515463917526\n",
      "(0.9926873857404022, 0.8350515463917526)\n",
      "\n",
      "\n",
      "The training F1-score-micro for RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) is: 0.9954296160877514\n",
      "The testing F1-score-micro for RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) is: 0.8298969072164948\n",
      "(0.9954296160877514, 0.8298969072164948)\n",
      "\n",
      "\n",
      "The training F1-score-micro for AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None) is: 0.4945155393053016\n",
      "The testing F1-score-micro for AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None) is: 0.44329896907216493\n",
      "(0.4945155393053016, 0.44329896907216493)\n",
      "\n",
      "\n",
      "The training F1-score-micro for SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False) is: 0.9396709323583181\n",
      "The testing F1-score-micro for SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False) is: 0.8505154639175259\n",
      "(0.9396709323583181, 0.8505154639175259)\n"
     ]
    }
   ],
   "source": [
    "print(f1_scorer_micro(logreg_class_pca, Z_train, Z_test, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer_micro(knn_class_pca, Z_train, Z_test, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer_micro(cart_class_pca, Z_train, Z_test, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer_micro(bagged_class_pca, Z_train, Z_test, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer_micro(random_forest_class_pca, Z_train, Z_test, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer_micro(adaboost_class_pca, Z_train, Z_test, y_train, y_test))\n",
    "print()\n",
    "print()\n",
    "print(f1_scorer_micro(support_vector_class_pca, Z_train, Z_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1-Score Micro, classification models with default parameters with PCA**\n",
    "- LogisticRegression - (0.9725776965265083, 0.8556701030927835)\n",
    "- KNeighborsClassifier - (0.8510054844606947, 0.7938144329896907)\n",
    "- DecisionTreeClassifier - (1.0, 0.7731958762886598)\n",
    "- BaggingClassifier - (0.9926873857404022, 0.8350515463917526)\n",
    "- RandomForestClassifier - (0.9954296160877514, 0.8298969072164948)\n",
    "- AdaBoostClassifier - (0.4945155393053016, 0.44329896907216493)\n",
    "- SVC - (0.9396709323583181, 0.8505154639175259)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model in pca_reg_models:\n",
    "    #joblib.dump(model, ('Models/Raw_Models/PCA/clas/'+model.__class__.__name__+'.joblib'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
